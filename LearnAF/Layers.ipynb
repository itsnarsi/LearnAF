{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport utils.autograd as ag\\n\\n\\nfrom activations import *\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "import numpy as np\n",
    "\n",
    "import arrayfire as af\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''\n",
    "import utils.autograd as ag\n",
    "\n",
    "\n",
    "from activations import *\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self,output_dim,name = None,bias = True):\n",
    "        self.output_dim = output_dim\n",
    "        self.name = name\n",
    "        self.bias = bias\n",
    "        self.input_shape = None\n",
    "    def __call__(self,x):\n",
    "        self.input_shape = x.output_dim\n",
    "        self.upper_layer = x\n",
    "        x.lower_layer = self\n",
    "        return self\n",
    "    def build(self):\n",
    "        if self.input_shape is not None:\n",
    "            self.w = af.randu(self.input_shape,self.output_dim)/100\n",
    "        else:\n",
    "            raise Exception('Input Shape is None!')\n",
    "        if self.bias == True:\n",
    "            self.b = af.randu(self.output_dim)/100\n",
    "        if hasattr(self,'upper_layer'):\n",
    "            return self.upper_layer.build()\n",
    "    def fwd(self,x):\n",
    "        self.input_data = x\n",
    "        return self.w * x + self.b\n",
    "    def bwd(self,dx,optimizer):\n",
    "        self.w = optimizer(dx*self.input_data,w)\n",
    "        self.b = optimizer(dx,b)\n",
    "        return dx * self.input_data\n",
    "    def get_neurons(self):\n",
    "        print((self.input_shape,self.output_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_1D = np.arange(-4, 4,1, dtype=np.float32)\n",
    "x = Variable(x_1D,name='x')\n",
    "y = Constant(2)\n",
    "x1 = x * x\n",
    "x2 = x1 + x \n",
    "z = x2.grad([x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': array([-7., -5., -3., -1.,  1.,  3.,  5.,  7.], dtype=float32)}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Variable at 0x7f62edc6da90>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import arrayfire as af\n",
    "from collections import namedtuple\n",
    "\n",
    "Point = \"Dict[str, float]\"\n",
    "\n",
    "class AG:\n",
    "    def eval(self) -> float:\n",
    "        \n",
    "        return self._eval({})\n",
    "    \n",
    "    def _eval(self, cache: dict) -> float:\n",
    "\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def grad(self, variables) -> Point:\n",
    "        cache = {}\n",
    "        self._eval(cache)\n",
    "        G = {}\n",
    "        for i in range(len(variables)):\n",
    "            G[variables[i].name] = 0\n",
    "        self._grad(1, G, cache)\n",
    "        return G\n",
    "\n",
    "    def _grad(self, adjoint: float, gradient: Point, cache):\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return Add(self, other)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return Subtract(self, other)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return Multiply(self, other)\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return Divide(self, other)\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        return Pow(self, other)\n",
    "\n",
    "class Variable(AG):\n",
    "    def __init__(self,value,name=None):\n",
    "        self.value = value\n",
    "        self.name = name\n",
    "    def _eval(self, cache):\n",
    "        cache[id(self)] = self.value\n",
    "        return cache[id(self)]\n",
    "\n",
    "    def _grad(self, adjoint, gradient, cache):\n",
    "        gradient[self.name] += adjoint\n",
    "\n",
    "class Constant(AG, namedtuple(\"Constant\", [\"value\"])):\n",
    "    def _eval(self, cache):\n",
    "        cache[id(self)] = self.value\n",
    "        return self.value\n",
    "\n",
    "    def _grad(self, ajoint, gradient, cache):\n",
    "        pass\n",
    "\n",
    "class Add(AG, namedtuple(\"Add\", [\"AG1\", \"AG2\"])):\n",
    "    def _eval(self, cache):\n",
    "        if id(self) not in cache:\n",
    "            eval1, eval2 = self.AG1._eval, self.AG2._eval\n",
    "            cache[id(self)] = eval1(cache) + eval2(cache)\n",
    "        return cache[id(self)]\n",
    "\n",
    "    def _grad(self, adjoint, gradient, cache):\n",
    "        self.AG1._grad(adjoint, gradient, cache)\n",
    "        self.AG2._grad(adjoint, gradient, cache)\n",
    "\n",
    "class Subtract(AG, namedtuple(\"Subtract\", [\"AG1\", \"AG2\"])):\n",
    "    def _eval(self, cache):\n",
    "        if id(self) not in cache:\n",
    "            eval1, eval2 = self.AG1._eval, self.AG2._eval\n",
    "            cache[id(self)] = eval1(cache) - eval2(cache)\n",
    "        return cache[id(self)]\n",
    "\n",
    "    def _grad(self, adjoint, gradient, cache):\n",
    "        self.AG1._grad(adjoint, gradient, cache)\n",
    "        self.AG2._grad(-adjoint, gradient, cache)\n",
    "\n",
    "class Multiply(AG, namedtuple(\"Multiply\", [\"AG1\", \"AG2\"])):\n",
    "    def _eval(self, cache):\n",
    "        if id(self) not in cache:\n",
    "            eval1, eval2 = self.AG1._eval, self.AG2._eval\n",
    "            cache[id(self)] = eval1(cache) * eval2(cache)\n",
    "        return cache[id(self)]\n",
    "\n",
    "    def _grad(self, adjoint, gradient, cache):\n",
    "        lhs = cache[id(self.AG1)]\n",
    "        rhs = cache[id(self.AG2)]\n",
    "        self.AG1._grad(adjoint * rhs, gradient, cache)\n",
    "        self.AG2._grad(adjoint * lhs, gradient, cache)\n",
    "\n",
    "class Divide(AG, namedtuple(\"Divide\", [\"AG1\", \"AG2\"])):\n",
    "    def _eval(self, cache):\n",
    "        if id(self) not in cache:\n",
    "            eval1, eval2 = self.AG1._eval, self.AG2._eval\n",
    "            cache[id(self)] = eval1(cache) / eval2(cache)\n",
    "        return cache[id(self)]\n",
    "\n",
    "    def _grad(self, adjoint, gradient, cache):\n",
    "        high = cache[id(self.AG1)]\n",
    "        low = cache[id(self.AG2)]\n",
    "        self.AG1._grad(adjoint / low, gradient, cache)\n",
    "        self.AG2._grad(-adjoint * high / low ** 2, gradient,\n",
    "                                 cache)\n",
    "\n",
    "class Pow(AG, namedtuple(\"Pow\", [\"AG1\", \"AG2\"])):\n",
    "    def _eval(self, cache):\n",
    "        if id(self) not in cache:\n",
    "            eval1, eval2 = self.AG1._eval, self.AG2._eval\n",
    "            cache[id(self)] = eval1(cache) ** eval2(cache)\n",
    "        return cache[id(self)]\n",
    "    def _grad(self, adjoint, gradient, cache):\n",
    "        base = cache[id(self.AG1)]\n",
    "        exp = cache[id(self.AG2)]\n",
    "\n",
    "        self.AG1._grad(adjoint * exp * base ** (exp - 1), gradient, cache)\n",
    "        self.AG2._grad(adjoint * af.arith.log(base) * base ** exp, gradient, cache)\n",
    "\n",
    "class sin(AG, namedtuple(\"sin\", [\"AG1\"])):\n",
    "    def _eval(self, cache):\n",
    "        if id(self) not in cache:\n",
    "            eval1 = self.AG1._eval\n",
    "            cache[id(self)] = af.arith.sin(eval1(cache))\n",
    "        return cache[id(self)]\n",
    "\n",
    "    def _grad(self, adjoint, gradient, cache):\n",
    "        a = cache[id(self.AG1)]\n",
    "        self.AG1._grad(adjoint * af.arith.cos(a), gradient, cache)\n",
    "        \n",
    "        \n",
    "class cos(AG, namedtuple(\"cos\", [\"AG1\"])):\n",
    "    def _eval(self, cache):\n",
    "        if id(self) not in cache:\n",
    "            eval1 = self.AG1._eval\n",
    "            cache[id(self)] = af.arith.cos(eval1(cache))\n",
    "        return cache[id(self)]\n",
    "\n",
    "    def _grad(self, adjoint, gradient, cache):\n",
    "        a = cache[id(self.AG1)]\n",
    "        self.AG1._grad(-adjoint * af.arith.sin(a), gradient, cache)\n",
    "\n",
    "\n",
    "class exp(AG, namedtuple(\"exp\", [\"AG1\"])):\n",
    "    def _eval(self, cache):\n",
    "        if id(self) not in cache:\n",
    "            eval1 = self.AG1._eval\n",
    "            cache[id(self)] = af.arith.exp(eval1(cache))\n",
    "        return cache[id(self)]\n",
    "\n",
    "    def _grad(self, adjoint, gradient, cache):\n",
    "        a = cache[id(self.AG1)]\n",
    "        self.AG1._grad(adjoint * af.arith.exp(a), gradient, cache)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 5)\n"
     ]
    }
   ],
   "source": [
    "D2.lower_layer.get_neurons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dense' object has no attribute 'w'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-5168803832c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dense' object has no attribute 'w'"
     ]
    }
   ],
   "source": [
    "D.w"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
